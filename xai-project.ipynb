{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nGeneral utils for training, evaluation and data loading\n\"\"\"\nimport os\nimport torch\nimport pickle\nimport numpy as np\nimport torchvision.transforms as transforms\n\nfrom PIL import Image\n\nfrom torch.utils.data import BatchSampler\nfrom torch.utils.data import Dataset, DataLoader\n\n# General\nBASE_DIR = ''\nN_ATTRIBUTES = 312\nN_CLASSES = 200\n\n# Training\nUPWEIGHT_RATIO = 9.0\nMIN_LR = 0.0001\nLR_DECAY_SIZE = 0.1\n\nclass CUBDataset(Dataset):\n    \"\"\"\n    Returns a compatible Torch Dataset object customized for the CUB dataset\n    \"\"\"\n\n    def __init__(self, pkl_file_paths, use_attr, no_img, uncertain_label, image_dir, n_class_attr, transform=None):\n        \"\"\"\n        Arguments:\n        pkl_file_paths: list of full path to all the pkl data\n        use_attr: whether to load the attributes (e.g. False for simple finetune)\n        no_img: whether to load the images (e.g. False for A -> Y model)\n        uncertain_label: if True, use 'uncertain_attribute_label' field (i.e. label weighted by uncertainty score, e.g. 1 & 3(probably) -> 0.75)\n        image_dir: default = 'images'. Will be append to the parent dir\n        n_class_attr: number of classes to predict for each attribute. If 3, then make a separate class for not visible\n        transform: whether to apply any special transformation. Default = None, i.e. use standard ImageNet preprocessing\n        \"\"\"\n        self.data = []\n        self.is_train = any([\"train\" in path for path in pkl_file_paths])\n        if not self.is_train:\n            assert any([(\"test\" in path) or (\"val\" in path) for path in pkl_file_paths])\n        for file_path in pkl_file_paths:\n            self.data.extend(pickle.load(open(file_path, 'rb')))\n        self.transform = transform\n        self.use_attr = use_attr\n        self.no_img = no_img\n        self.uncertain_label = uncertain_label\n        self.image_dir = image_dir\n        self.n_class_attr = n_class_attr\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_data = self.data[idx]\n        img_path = img_data['img_path']\n        \n        # Trim unnecessary paths\n        try:\n            idx = img_path.split('/').index('CUB_200_2011')\n            if self.image_dir != 'images':\n                img_path = '/'.join([self.image_dir] + img_path.split('/')[idx+1:])\n                img_path = img_path.replace('images/', '')\n            else:\n                img_path = '/'.join(img_path.split('/')[idx:])\n            img = Image.open(img_path).convert('RGB')\n        except:\n            #img_path_split = img_path.split('/')\n            #split = 'train' if self.is_train else 'test'\n            #img_path = '/'.join(img_path_split[:2] + [split] + img_path_split[2:])\n            \n            img = Image.open(img_path).convert('RGB')\n\n        class_label = img_data['class_label']\n        if self.transform:\n            img = self.transform(img)\n\n        if self.use_attr:\n            if self.uncertain_label:\n                attr_label = img_data['uncertain_attribute_label']\n            else:\n                attr_label = img_data['attribute_label']\n            if self.no_img:\n                if self.n_class_attr == 3:\n                    one_hot_attr_label = np.zeros((N_ATTRIBUTES, self.n_class_attr))\n                    one_hot_attr_label[np.arange(N_ATTRIBUTES), attr_label] = 1\n                    return one_hot_attr_label, class_label\n                else:\n                    return attr_label, class_label\n            else:\n                return img, class_label, attr_label\n        else:\n            return img, class_label\n\n\nclass ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n    Arguments:\n        indices (list, optional): a list of indices\n        num_samples (int, optional): number of samples to draw\n    \"\"\"\n\n    def __init__(self, dataset, indices=None):\n        # if indices is not provided,\n        # all elements in the dataset will be considered\n        self.indices = list(range(len(dataset))) \\\n            if indices is None else indices\n\n        # if num_samples is not provided,\n        # draw `len(indices)` samples in each iteration\n        self.num_samples = len(self.indices)\n\n        # distribution of classes in the dataset\n        label_to_count = {}\n        for idx in self.indices:\n            label = self._get_label(dataset, idx)\n            if label in label_to_count:\n                label_to_count[label] += 1\n            else:\n                label_to_count[label] = 1\n\n        # weight for each sample\n        weights = [1.0 / label_to_count[self._get_label(dataset, idx)]\n                   for idx in self.indices]\n        self.weights = torch.DoubleTensor(weights)\n\n    def _get_label(self, dataset, idx):  # Note: for single attribute dataset\n        return dataset.data[idx]['attribute_label'][0]\n\n    def __iter__(self):\n        idx = (self.indices[i] for i in torch.multinomial(\n            self.weights, self.num_samples, replacement=True))\n        return idx\n\n    def __len__(self):\n        return self.num_samples\n\ndef load_data(pkl_paths, use_attr, no_img, batch_size, uncertain_label=False, n_class_attr=2, image_dir='images', resampling=False, resol=299):\n    \"\"\"\n    Note: Inception needs (299,299,3) images with inputs scaled between -1 and 1\n    Loads data with transformations applied, and upsample the minority class if there is class imbalance and weighted loss is not used\n    NOTE: resampling is customized for first attribute only, so change sampler.py if necessary\n    \"\"\"\n    resized_resol = int(resol * 256/224)\n    is_training = any(['train.pkl' in f for f in pkl_paths])\n    if is_training:\n        transform = transforms.Compose([\n            #transforms.Resize((resized_resol, resized_resol)),\n            #transforms.RandomSizedCrop(resol),\n            transforms.ColorJitter(brightness=32/255, saturation=(0.5, 1.5)),\n            transforms.RandomResizedCrop(resol),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(), #implicitly divides by 255\n            transforms.Normalize(mean = [0.5, 0.5, 0.5], std = [2, 2, 2])\n            #transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ], std = [ 0.229, 0.224, 0.225 ]),\n            ])\n    else:\n        transform = transforms.Compose([\n            #transforms.Resize((resized_resol, resized_resol)),\n            transforms.CenterCrop(resol),\n            transforms.ToTensor(), #implicitly divides by 255\n            transforms.Normalize(mean = [0.5, 0.5, 0.5], std = [2, 2, 2])\n            #transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ], std = [ 0.229, 0.224, 0.225 ]),\n            ])\n\n    dataset = CUBDataset(pkl_paths, use_attr, no_img, uncertain_label, image_dir, n_class_attr, transform)\n    if is_training:\n        drop_last = True\n        shuffle = True\n    else:\n        drop_last = False\n        shuffle = False\n    if resampling:\n        sampler = BatchSampler(ImbalancedDatasetSampler(dataset), batch_size=batch_size, drop_last=drop_last)\n        loader = DataLoader(dataset, batch_sampler=sampler)\n    else:\n        loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n    return loader\n\ndef find_class_imbalance(pkl_file, multiple_attr=False, attr_idx=-1):\n    \"\"\"\n    Calculate class imbalance ratio for binary attribute labels stored in pkl_file\n    If attr_idx >= 0, then only return ratio for the corresponding attribute id\n    If multiple_attr is True, then return imbalance ratio separately for each attribute. Else, calculate the overall imbalance across all attributes\n    \"\"\"\n    imbalance_ratio = []\n    data = pickle.load(open(os.path.join(BASE_DIR, pkl_file), 'rb'))\n    n = len(data)\n    n_attr = len(data[0]['attribute_label'])\n    if attr_idx >= 0:\n        n_attr = 1\n    if multiple_attr:\n        n_ones = [0] * n_attr\n        total = [n] * n_attr\n    else:\n        n_ones = [0]\n        total = [n * n_attr]\n    for d in data:\n        labels = d['attribute_label']\n        if multiple_attr:\n            for i in range(n_attr):\n                n_ones[i] += labels[i]\n        else:\n            if attr_idx >= 0:\n                n_ones[0] += labels[attr_idx]\n            else:\n                n_ones[0] += sum(labels)\n    for j in range(len(n_ones)):\n        imbalance_ratio.append(total[j]/n_ones[j] - 1)\n    if not multiple_attr: #e.g. [9.0] --> [9.0] * 312\n        imbalance_ratio *= n_attr\n    return imbalance_ratio","metadata":{"execution":{"iopub.status.busy":"2023-03-31T11:02:17.500863Z","iopub.execute_input":"2023-03-31T11:02:17.501396Z","iopub.status.idle":"2023-03-31T11:02:17.533505Z","shell.execute_reply.started":"2023-03-31T11:02:17.501339Z","shell.execute_reply":"2023-03-31T11:02:17.532240Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"!pip install gdown","metadata":{"execution":{"iopub.status.busy":"2023-03-31T11:02:17.747772Z","iopub.execute_input":"2023-03-31T11:02:17.748089Z","iopub.status.idle":"2023-03-31T11:02:28.357336Z","shell.execute_reply.started":"2023-03-31T11:02:17.748059Z","shell.execute_reply":"2023-03-31T11:02:28.356071Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Requirement already satisfied: gdown in /opt/conda/lib/python3.7/site-packages (4.7.1)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from gdown) (4.11.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from gdown) (4.64.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gdown) (1.16.0)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.7/site-packages (from gdown) (2.28.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from gdown) (3.9.0)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (3.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.26.14)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.7.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!gdown 1DUkovCVCUqYScle624llHyeUuC7t0SaN\n!gdown 1oRBQ7WY_9-qfWkxNC7ZGBW27wecfEy6f\n!gdown 1EsYvhX6aRDELjoKpcRo3fuLo-aKF6DI9\n!gdown 1ir5HukW2XO25GWqx3jV-gKZWWi6Zg_rw","metadata":{"execution":{"iopub.status.busy":"2023-03-31T11:04:21.461430Z","iopub.execute_input":"2023-03-31T11:04:21.462519Z","iopub.status.idle":"2023-03-31T11:04:35.007367Z","shell.execute_reply.started":"2023-03-31T11:04:21.462474Z","shell.execute_reply":"2023-03-31T11:04:35.006150Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Downloading...\nFrom (uriginal): https://drive.google.com/uc?id=1DUkovCVCUqYScle624llHyeUuC7t0SaN\nFrom (redirected): https://drive.google.com/uc?id=1DUkovCVCUqYScle624llHyeUuC7t0SaN&confirm=t&uuid=292d9096-9ab7-41e1-9c5d-3772b4ebebc9\nTo: /kaggle/working/images.zip\n100%|███████████████████████████████████████| 1.13G/1.13G [00:04<00:00, 268MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1oRBQ7WY_9-qfWkxNC7ZGBW27wecfEy6f\nTo: /kaggle/working/kaggle_train.pkl\n100%|█████████████████████████████████████████| 497k/497k [00:00<00:00, 104MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1EsYvhX6aRDELjoKpcRo3fuLo-aKF6DI9\nTo: /kaggle/working/kaggle_test.pkl\n100%|████████████████████████████████████████| 601k/601k [00:00<00:00, 65.3MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1ir5HukW2XO25GWqx3jV-gKZWWi6Zg_rw\nTo: /kaggle/working/kaggle_val.pkl\n100%|████████████████████████████████████████| 124k/124k [00:00<00:00, 69.5MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import zipfile\n\nwith zipfile.ZipFile('./images.zip',\"r\") as z:\n    z.extractall()","metadata":{"execution":{"iopub.status.busy":"2023-03-31T11:02:43.208885Z","iopub.execute_input":"2023-03-31T11:02:43.209280Z","iopub.status.idle":"2023-03-31T11:02:53.905321Z","shell.execute_reply.started":"2023-03-31T11:02:43.209236Z","shell.execute_reply":"2023-03-31T11:02:53.903866Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Dataset loader from bottleneck concept script","metadata":{}},{"cell_type":"code","source":"\n\ntrain_data_path=\"/kaggle/working/kaggle_train.pkl\"\nval_data_path=\"/kaggle/working/kaggle_val.pkl\"\n\ntrain_loader = load_data([train_data_path], use_attr=False, no_img=False, batch_size=64, uncertain_label=False, image_dir=\"\", n_class_attr=0)\nval_loader = load_data([val_data_path], use_attr=False, no_img=False, batch_size=64, uncertain_label=False, image_dir=\"\", n_class_attr=0)","metadata":{"execution":{"iopub.status.busy":"2023-03-31T11:05:03.903956Z","iopub.execute_input":"2023-03-31T11:05:03.904715Z","iopub.status.idle":"2023-03-31T11:05:03.920174Z","shell.execute_reply.started":"2023-03-31T11:05:03.904671Z","shell.execute_reply":"2023-03-31T11:05:03.919138Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\n\n# Load the pre-trained ResNet18 model\nmodel = models.resnet18(weights='DEFAULT')\n\n# Modify the last layer to match the number of classes in the CUB dataset\nnum_classes = 200\nmodel.fc = nn.Linear(model.fc.in_features, num_classes)\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score\n\n# Define the device to use for computation (GPU if available, else CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n\n\n# Define the number of epochs to train for\nnum_epochs = 50\n\n# Train the model\nfor epoch in range(num_epochs):\n    # Set the model to training mode\n    model.train()\n\n    # Train one epoch\n    train_loss = 0.0\n    train_acc = 0.0\n    train_size = 0\n    for images, labels in train_loader:\n        # Move the data to the device\n        model = model.to(device)\n        images, labels = images.to(device), labels.to(device)\n\n        # Zero the gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(images)\n        loss = loss_fn(outputs, labels)\n\n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n\n        # Update the training loss and accuracy\n        train_loss += loss.item() * images.size(0)\n        _, preds = torch.max(outputs, 1)\n        train_acc += accuracy_score(preds.cpu().numpy(), labels.cpu().numpy()) * images.size(0)\n        train_size += images.size(0)\n\n    train_loss /= train_size\n    train_acc /= train_size\n\n    # Set the model to evaluation mode\n    model.eval()\n    model = model.to(device)\n\n    # Evaluate one epoch\n    val_loss = 0.0\n    val_acc = 0.0\n    val_size = 0\n    with torch.no_grad():\n        for images, labels in val_loader:\n            # Move the data to the device\n            images, labels = images.to(device), labels.to(device)\n\n            # Forward pass\n            outputs = model(images)\n            loss = loss_fn(outputs, labels)\n\n            # Update the validation loss and accuracy\n            val_loss += loss.item() * images.size(0)\n            _, preds = torch.max(outputs, 1)\n            val_acc += accuracy_score(preds.cpu().numpy(), labels.cpu().numpy()) * images.size(0)\n            val_size += images.size(0)\n\n    val_loss /= val_size\n    val_acc /= val_size\n\n    # Print the results for this epoch\n    print(f\"Epoch {epoch + 1}/{num_epochs}: \"\n          f\"train_loss={train_loss:.4f} \"\n          f\"train_acc={train_acc:.4f} \"\n          f\"val_loss={val_loss:.4f} \"\n          f\"val_acc={val_acc:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-31T11:06:47.748249Z","iopub.execute_input":"2023-03-31T11:06:47.748645Z","iopub.status.idle":"2023-03-31T12:11:27.423947Z","shell.execute_reply.started":"2023-03-31T11:06:47.748610Z","shell.execute_reply":"2023-03-31T12:11:27.422611Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Epoch 1/50: train_loss=5.0298 train_acc=0.0304 val_loss=4.6063 val_acc=0.0551\nEpoch 2/50: train_loss=3.9657 train_acc=0.1147 val_loss=4.3824 val_acc=0.1018\nEpoch 3/50: train_loss=3.3783 train_acc=0.1900 val_loss=3.3361 val_acc=0.1995\nEpoch 4/50: train_loss=2.9120 train_acc=0.2755 val_loss=2.9075 val_acc=0.2671\nEpoch 5/50: train_loss=2.5807 train_acc=0.3465 val_loss=2.4349 val_acc=0.3539\nEpoch 6/50: train_loss=2.3228 train_acc=0.3967 val_loss=2.5909 val_acc=0.3673\nEpoch 7/50: train_loss=2.1325 train_acc=0.4597 val_loss=2.3854 val_acc=0.3831\nEpoch 8/50: train_loss=1.9406 train_acc=0.5076 val_loss=2.2273 val_acc=0.4124\nEpoch 9/50: train_loss=1.7853 train_acc=0.5401 val_loss=2.1069 val_acc=0.4549\nEpoch 10/50: train_loss=1.6715 train_acc=0.5669 val_loss=1.8365 val_acc=0.5125\nEpoch 11/50: train_loss=1.5828 train_acc=0.5864 val_loss=1.9028 val_acc=0.5109\nEpoch 12/50: train_loss=1.5039 train_acc=0.6087 val_loss=1.6610 val_acc=0.5559\nEpoch 13/50: train_loss=1.3941 train_acc=0.6387 val_loss=1.6531 val_acc=0.5651\nEpoch 14/50: train_loss=1.3407 train_acc=0.6440 val_loss=1.6957 val_acc=0.5568\nEpoch 15/50: train_loss=1.2808 train_acc=0.6653 val_loss=1.8417 val_acc=0.5317\nEpoch 16/50: train_loss=1.2264 train_acc=0.6829 val_loss=1.6422 val_acc=0.5918\nEpoch 17/50: train_loss=1.1813 train_acc=0.6862 val_loss=1.7107 val_acc=0.5785\nEpoch 18/50: train_loss=1.0917 train_acc=0.7061 val_loss=1.5747 val_acc=0.5902\nEpoch 19/50: train_loss=1.0706 train_acc=0.7232 val_loss=1.5683 val_acc=0.5985\nEpoch 20/50: train_loss=1.0308 train_acc=0.7280 val_loss=1.6605 val_acc=0.5710\nEpoch 21/50: train_loss=1.0173 train_acc=0.7397 val_loss=1.4395 val_acc=0.6386\nEpoch 22/50: train_loss=0.9690 train_acc=0.7517 val_loss=1.5141 val_acc=0.6369\nEpoch 23/50: train_loss=0.9626 train_acc=0.7525 val_loss=1.5629 val_acc=0.6060\nEpoch 24/50: train_loss=0.8823 train_acc=0.7701 val_loss=1.6758 val_acc=0.5826\nEpoch 25/50: train_loss=0.8931 train_acc=0.7713 val_loss=1.5603 val_acc=0.6411\nEpoch 26/50: train_loss=0.9082 train_acc=0.7658 val_loss=1.5355 val_acc=0.6110\nEpoch 27/50: train_loss=0.8464 train_acc=0.7836 val_loss=1.8456 val_acc=0.5659\nEpoch 28/50: train_loss=0.8017 train_acc=0.7943 val_loss=1.5712 val_acc=0.6068\nEpoch 29/50: train_loss=0.8075 train_acc=0.7988 val_loss=1.7173 val_acc=0.6010\nEpoch 30/50: train_loss=0.7930 train_acc=0.7946 val_loss=1.6308 val_acc=0.6085\nEpoch 31/50: train_loss=0.7544 train_acc=0.7996 val_loss=1.5905 val_acc=0.6319\nEpoch 32/50: train_loss=0.7492 train_acc=0.8104 val_loss=1.6539 val_acc=0.6361\nEpoch 33/50: train_loss=0.7559 train_acc=0.8057 val_loss=1.5844 val_acc=0.6419\nEpoch 34/50: train_loss=0.7542 train_acc=0.8045 val_loss=1.7768 val_acc=0.6035\nEpoch 35/50: train_loss=0.7289 train_acc=0.8148 val_loss=1.6630 val_acc=0.6219\nEpoch 36/50: train_loss=0.6960 train_acc=0.8233 val_loss=1.5626 val_acc=0.6227\nEpoch 37/50: train_loss=0.6712 train_acc=0.8309 val_loss=1.8174 val_acc=0.6177\nEpoch 38/50: train_loss=0.7047 train_acc=0.8174 val_loss=1.6684 val_acc=0.6386\nEpoch 39/50: train_loss=0.6675 train_acc=0.8285 val_loss=1.6391 val_acc=0.6269\nEpoch 40/50: train_loss=0.6301 train_acc=0.8414 val_loss=1.7526 val_acc=0.6210\nEpoch 41/50: train_loss=0.6330 train_acc=0.8452 val_loss=1.7125 val_acc=0.6152\nEpoch 42/50: train_loss=0.6473 train_acc=0.8366 val_loss=1.5323 val_acc=0.6569\nEpoch 43/50: train_loss=0.6246 train_acc=0.8410 val_loss=1.6059 val_acc=0.6319\nEpoch 44/50: train_loss=0.6737 train_acc=0.8260 val_loss=1.6950 val_acc=0.6336\nEpoch 45/50: train_loss=0.6666 train_acc=0.8332 val_loss=1.7362 val_acc=0.6311\nEpoch 46/50: train_loss=0.6318 train_acc=0.8376 val_loss=1.6884 val_acc=0.6336\nEpoch 47/50: train_loss=0.6316 train_acc=0.8442 val_loss=1.7209 val_acc=0.6427\nEpoch 48/50: train_loss=0.6249 train_acc=0.8393 val_loss=1.5959 val_acc=0.6511\nEpoch 49/50: train_loss=0.5912 train_acc=0.8488 val_loss=1.9002 val_acc=0.6185\nEpoch 50/50: train_loss=0.6022 train_acc=0.8459 val_loss=1.6573 val_acc=0.6486\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'resnet18_trained.pth')","metadata":{"execution":{"iopub.status.busy":"2023-03-31T12:13:40.957510Z","iopub.execute_input":"2023-03-31T12:13:40.958379Z","iopub.status.idle":"2023-03-31T12:13:41.034158Z","shell.execute_reply.started":"2023-03-31T12:13:40.958319Z","shell.execute_reply":"2023-03-31T12:13:41.033006Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\n\nmodel = models.resnet18()\nnum_classes = 200\nmodel.fc = nn.Linear(model.fc.in_features, num_classes)\nmodel.load_state_dict(torch.load('resnet18_trained.pth'))\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-31T12:13:41.945298Z","iopub.execute_input":"2023-03-31T12:13:41.945981Z","iopub.status.idle":"2023-03-31T12:13:42.176177Z","shell.execute_reply.started":"2023-03-31T12:13:41.945946Z","shell.execute_reply":"2023-03-31T12:13:42.174970Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom torch.autograd import Variable\n\ndef compute_saliency_maps(images, labels, model):\n    # Make sure the model is in evaluation mode\n    model.eval()\n\n    # Convert the images and labels to variables\n    images = Variable(images, requires_grad=True)\n    labels = Variable(labels)\n\n    # Forward pass\n    output = model(images)\n    loss = torch.nn.functional.cross_entropy(output, labels)\n\n    # Backward pass\n    loss.backward()\n\n    # Get the gradients of the input with respect to the loss\n    gradients = images.grad.data\n\n    # Absolute value of the gradients\n    gradients = torch.abs(gradients)\n\n    # Max of the gradients across the channels\n    gradients, _ = torch.max(gradients, dim=1)\n\n    # Normalize the gradients\n    gradients = torch.div(gradients, torch.max(gradients))\n\n    return gradients","metadata":{"execution":{"iopub.status.busy":"2023-03-31T12:13:44.160765Z","iopub.execute_input":"2023-03-31T12:13:44.161844Z","iopub.status.idle":"2023-03-31T12:13:44.169861Z","shell.execute_reply.started":"2023-03-31T12:13:44.161797Z","shell.execute_reply":"2023-03-31T12:13:44.168606Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy \nimport cv2\n\ntest_data_path = \"/kaggle/working/kaggle_test.pkl\"\ntest_loader = load_data([test_data_path], use_attr=False, no_img=False, batch_size=64, uncertain_label=False, image_dir=\"\", n_class_attr=0)\n# Choose a few images from the test set\nimg_list=[]\n\nfor images, labels in test_loader:\n\n# Compute the saliency maps\n    saliency_maps = compute_saliency_maps(images, labels, model)\n\n# Plot the original images and their saliency maps\nfor i in range(5):\n    # Convert the saliency map to a heatmap and normalize it between 0 and 1\n    heatmap = np.uint8(255 * saliency_maps[i]/np.max(saliency_maps[i]))\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    heatmap = np.float32(heatmap) / 255\n\n    # Resize the heatmap to the size of the original image\n    heatmap = cv2.resize(heatmap, (images.size(-1), images.size(-2)))\n    heatmap = np.transpose(heatmap, (2, 0, 1))\n\n    # Overlay the heatmap on the original image\n    cam = heatmap + np.float32(images[i].squeeze().numpy().transpose((1, 2, 0)))\n    cam = cam / np.max(cam)\n    plt.figure()\n    plt.subplot(121)\n    plt.imshow(images[i].permute(1, 2, 0))\n    plt.axis('off')\n    plt.subplot(122)\n    plt.imshow(cam)\n    plt.axis('off')\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-03-31T12:13:45.068263Z","iopub.execute_input":"2023-03-31T12:13:45.069709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the original images and their saliency maps\n\n#saliency_maps=saliency_maps.numpy().transpose(1, 2, 0)\n\nsaliency_maps=saliency_maps.transpose(1, 2, 0)\n\nfor i in range(5):\n    # Convert the saliency map to a heatmap and normalize it between 0 and 1\n    heatmap = np.uint8(255 * saliency_maps[:,:, i]/np.max(saliency_maps[:,:, i]))\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    heatmap = np.float32(heatmap) / 255\n\n    # Resize the heatmap to the size of the original image\n    heatmap = cv2.resize(heatmap, (images.size(-1), images.size(-2)))\n    #heatmap = np.transpose(heatmap, (2, 0, 1))\n\n    # Overlay the heatmap on the original image\n    cam = heatmap + np.float32(images[i].squeeze().numpy().transpose((1, 2, 0)))\n    cam = cam / np.max(cam)\n    plt.figure()\n    plt.subplot(121)\n    plt.imshow(images[i].permute(1, 2, 0))\n    plt.axis('off')\n    plt.subplot(122)\n    plt.imshow(cam)\n    plt.axis('off')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"saliency_maps[:,:, i].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}