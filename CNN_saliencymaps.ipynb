{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zuzal\\anaconda3\\envs\\mlops\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define transform to resize images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((480, 480)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, filename, transform=None):\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[idx]['img_path']\n",
    "        label = self.data[idx]['class_label']\n",
    "        \n",
    "        # Load PIL image\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        # Apply transform\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "                # Check image size\n",
    "        if img.size != (3, 480, 480):\n",
    "            print(img.size)\n",
    "            return None\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "# Create dataloader object\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "\n",
    "train_set = CustomDataset(filename='train.pkl', transform=transform)\n",
    "test_set = CustomDataset(filename='test.pkl', transform=transform)\n",
    "val_set = CustomDataset(filename='val.pkl', transform=transform)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset loader from bottleneck concept script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_data\n",
    "\n",
    "train_data_path=r\"C:\\Users\\zuzal\\Masters\\Responsible_AI\\XAI\\CNN\\train.pkl\"\n",
    "val_data_path=r\"C:\\Users\\zuzal\\Masters\\Responsible_AI\\XAI\\CNN\\val.pkl\"\n",
    "\n",
    "train_loader = load_data([train_data_path], use_attr=False, no_img=False, batch_size=64, uncertain_label=False, image_dir=\"\", n_class_attr=0)\n",
    "val_loader = load_data([val_data_path], use_attr=False, no_img=False, batch_size=64, uncertain_label=False, image_dir=\"\", n_class_attr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zuzal\\anaconda3\\envs\\mlops\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: train_loss=4.9722 train_acc=0.0355 val_loss=4.5012 val_acc=0.0518\n",
      "Epoch 2/10: train_loss=4.0221 train_acc=0.1106 val_loss=4.2354 val_acc=0.1035\n",
      "Epoch 3/10: train_loss=3.3970 train_acc=0.1993 val_loss=3.2840 val_acc=0.1811\n",
      "Epoch 4/10: train_loss=2.9636 train_acc=0.2684 val_loss=3.3148 val_acc=0.2295\n",
      "Epoch 5/10: train_loss=2.6713 train_acc=0.3347 val_loss=2.6165 val_acc=0.3189\n",
      "Epoch 6/10: train_loss=2.3612 train_acc=0.4035 val_loss=3.4322 val_acc=0.2396\n",
      "Epoch 7/10: train_loss=2.1324 train_acc=0.4590 val_loss=2.2228 val_acc=0.4265\n",
      "Epoch 8/10: train_loss=1.9590 train_acc=0.4996 val_loss=1.8463 val_acc=0.4942\n",
      "Epoch 9/10: train_loss=1.8183 train_acc=0.5370 val_loss=2.0371 val_acc=0.4674\n",
      "Epoch 10/10: train_loss=1.7394 train_acc=0.5566 val_loss=1.8945 val_acc=0.5100\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load the pre-trained ResNet18 model\n",
    "model = models.resnet18(weights='DEFAULT')\n",
    "\n",
    "# Modify the last layer to match the number of classes in the CUB dataset\n",
    "num_classes = 200\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the device to use for computation (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "\n",
    "# Define the number of epochs to train for\n",
    "num_epochs = 10\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Train one epoch\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    train_size = 0\n",
    "    for images, labels in train_loader:\n",
    "        # Move the data to the device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the training loss and accuracy\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_acc += accuracy_score(preds.cpu().numpy(), labels.cpu().numpy()) * images.size(0)\n",
    "        train_size += images.size(0)\n",
    "\n",
    "    train_loss /= train_size\n",
    "    train_acc /= train_size\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Evaluate one epoch\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_size = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            # Move the data to the device\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            # Update the validation loss and accuracy\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_acc += accuracy_score(preds.cpu().numpy(), labels.cpu().numpy()) * images.size(0)\n",
    "            val_size += images.size(0)\n",
    "\n",
    "    val_loss /= val_size\n",
    "    val_acc /= val_size\n",
    "\n",
    "    # Print the results for this epoch\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}: \"\n",
    "          f\"train_loss={train_loss:.4f} \"\n",
    "          f\"train_acc={train_acc:.4f} \"\n",
    "          f\"val_loss={val_loss:.4f} \"\n",
    "          f\"val_acc={val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'resnet18_trained.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "model = models.resnet18()\n",
    "num_classes = 200\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model.load_state_dict(torch.load('resnet18_trained.pth'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def compute_saliency_maps(images, labels, model):\n",
    "    # Make sure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Convert the images and labels to variables\n",
    "    images = Variable(images, requires_grad=True)\n",
    "    labels = Variable(labels)\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(images)\n",
    "    loss = torch.nn.functional.cross_entropy(output, labels)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Get the gradients of the input with respect to the loss\n",
    "    gradients = images.grad.data\n",
    "\n",
    "    # Absolute value of the gradients\n",
    "    gradients = torch.abs(gradients)\n",
    "\n",
    "    # Max of the gradients across the channels\n",
    "    gradients, _ = torch.max(gradients, dim=1)\n",
    "\n",
    "    # Normalize the gradients\n",
    "    gradients = torch.div(gradients, torch.max(gradients))\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "max() received an invalid combination of arguments - got (axis=NoneType, out=NoneType, ), but expected one of:\n * ()\n * (Tensor other)\n * (int dim, bool keepdim)\n      didn't match because some of the keywords were incorrect: axis, out\n * (name dim, bool keepdim)\n      didn't match because some of the keywords were incorrect: axis, out\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m# Plot the original images and their saliency maps\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n\u001b[0;32m     17\u001b[0m     \u001b[39m# Convert the saliency map to a heatmap and normalize it between 0 and 1\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     heatmap \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39muint8(\u001b[39m255\u001b[39m \u001b[39m*\u001b[39m saliency_maps[i]\u001b[39m/\u001b[39mnp\u001b[39m.\u001b[39;49mmax(saliency_maps[i]))\n\u001b[0;32m     19\u001b[0m     heatmap \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mapplyColorMap(heatmap, cv2\u001b[39m.\u001b[39mCOLORMAP_JET)\n\u001b[0;32m     20\u001b[0m     heatmap \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfloat32(heatmap) \u001b[39m/\u001b[39m \u001b[39m255\u001b[39m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mamax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\zuzal\\anaconda3\\envs\\mlops\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2820\u001b[0m, in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2703\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_amax_dispatcher)\n\u001b[0;32m   2704\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mamax\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue, initial\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue,\n\u001b[0;32m   2705\u001b[0m          where\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue):\n\u001b[0;32m   2706\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2707\u001b[0m \u001b[39m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[0;32m   2708\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2818\u001b[0m \u001b[39m    5\u001b[39;00m\n\u001b[0;32m   2819\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2820\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapreduction(a, np\u001b[39m.\u001b[39;49mmaximum, \u001b[39m'\u001b[39;49m\u001b[39mmax\u001b[39;49m\u001b[39m'\u001b[39;49m, axis, \u001b[39mNone\u001b[39;49;00m, out,\n\u001b[0;32m   2821\u001b[0m                           keepdims\u001b[39m=\u001b[39;49mkeepdims, initial\u001b[39m=\u001b[39;49minitial, where\u001b[39m=\u001b[39;49mwhere)\n",
      "File \u001b[1;32mc:\\Users\\zuzal\\anaconda3\\envs\\mlops\\lib\\site-packages\\numpy\\core\\fromnumeric.py:84\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[0;32m     83\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[0;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m ufunc\u001b[39m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: max() received an invalid combination of arguments - got (axis=NoneType, out=NoneType, ), but expected one of:\n * ()\n * (Tensor other)\n * (int dim, bool keepdim)\n      didn't match because some of the keywords were incorrect: axis, out\n * (name dim, bool keepdim)\n      didn't match because some of the keywords were incorrect: axis, out\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy \n",
    "import cv2\n",
    "\n",
    "test_data_path = r\"C:\\Users\\zuzal\\Masters\\Responsible_AI\\XAI\\CNN\\test.pkl\"\n",
    "test_loader = load_data([test_data_path], use_attr=False, no_img=False, batch_size=64, uncertain_label=False, image_dir=\"\", n_class_attr=0)\n",
    "# Choose a few images from the test set\n",
    "img_list=[]\n",
    "\n",
    "for images, labels in test_loader:\n",
    "\n",
    "# Compute the saliency maps\n",
    "    saliency_maps = compute_saliency_maps(images, labels, model)\n",
    "\n",
    "# Plot the original images and their saliency maps\n",
    "for i in range(5):\n",
    "    # Convert the saliency map to a heatmap and normalize it between 0 and 1\n",
    "    heatmap = np.uint8(255 * saliency_maps[i]/np.max(saliency_maps[i]))\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "\n",
    "    # Resize the heatmap to the size of the original image\n",
    "    heatmap = cv2.resize(heatmap, (images.size(-1), images.size(-2)))\n",
    "    heatmap = np.transpose(heatmap, (2, 0, 1))\n",
    "\n",
    "    # Overlay the heatmap on the original image\n",
    "    cam = heatmap + np.float32(images[i].squeeze().numpy().transpose((1, 2, 0)))\n",
    "    cam = cam / np.max(cam)\n",
    "    plt.figure()\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(images[i].permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(cam)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,299,299) (299,299,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m heatmap \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtranspose(heatmap, (\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m     17\u001b[0m \u001b[39m# Overlay the heatmap on the original image\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m cam \u001b[39m=\u001b[39m heatmap \u001b[39m+\u001b[39;49m np\u001b[39m.\u001b[39;49mfloat32(images[i]\u001b[39m.\u001b[39;49msqueeze()\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39;49mtranspose((\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m0\u001b[39;49m)))\n\u001b[0;32m     19\u001b[0m cam \u001b[39m=\u001b[39m cam \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39mmax(cam)\n\u001b[0;32m     20\u001b[0m plt\u001b[39m.\u001b[39mfigure()\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,299,299) (299,299,3) "
     ]
    }
   ],
   "source": [
    "# Plot the original images and their saliency maps\n",
    "\n",
    "#saliency_maps=saliency_maps.numpy().transpose(1, 2, 0)\n",
    "\n",
    "saliency_maps=saliency_maps.transpose(1, 2, 0)\n",
    "\n",
    "for i in range(5):\n",
    "    # Convert the saliency map to a heatmap and normalize it between 0 and 1\n",
    "    heatmap = np.uint8(255 * saliency_maps[i]/np.max(saliency_maps[i]))\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "\n",
    "    # Resize the heatmap to the size of the original image\n",
    "    heatmap = cv2.resize(heatmap, (images.size(-1), images.size(-2)))\n",
    "    heatmap = np.transpose(heatmap, (2, 0, 1))\n",
    "\n",
    "    # Overlay the heatmap on the original image\n",
    "    cam = heatmap + np.float32(images[i].squeeze().numpy().transpose((1, 2, 0)))\n",
    "    cam = cam / np.max(cam)\n",
    "    plt.figure()\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(images[i].permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(cam)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(299, 299)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saliency_maps[:,:, i].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
