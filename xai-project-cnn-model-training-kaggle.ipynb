{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nGeneral utils for training, evaluation and data loading\n\"\"\"\nimport os\nimport torch\nimport pickle\nimport numpy as np\nimport torchvision.transforms as transforms\n\nfrom PIL import Image\n\nfrom torch.utils.data import BatchSampler\nfrom torch.utils.data import Dataset, DataLoader\n\n# General\nBASE_DIR = ''\nN_ATTRIBUTES = 312\nN_CLASSES = 200\n\n# Training\nUPWEIGHT_RATIO = 9.0\nMIN_LR = 0.0001\nLR_DECAY_SIZE = 0.1\n\nclass CUBDataset(Dataset):\n    \"\"\"\n    Returns a compatible Torch Dataset object customized for the CUB dataset\n    \"\"\"\n\n    def __init__(self, pkl_file_paths, use_attr, no_img, uncertain_label, image_dir, n_class_attr, transform=None):\n        \"\"\"\n        Arguments:\n        pkl_file_paths: list of full path to all the pkl data\n        use_attr: whether to load the attributes (e.g. False for simple finetune)\n        no_img: whether to load the images (e.g. False for A -> Y model)\n        uncertain_label: if True, use 'uncertain_attribute_label' field (i.e. label weighted by uncertainty score, e.g. 1 & 3(probably) -> 0.75)\n        image_dir: default = 'images'. Will be append to the parent dir\n        n_class_attr: number of classes to predict for each attribute. If 3, then make a separate class for not visible\n        transform: whether to apply any special transformation. Default = None, i.e. use standard ImageNet preprocessing\n        \"\"\"\n        self.data = []\n        self.is_train = any([\"train\" in path for path in pkl_file_paths])\n        if not self.is_train:\n            assert any([(\"test\" in path) or (\"val\" in path) for path in pkl_file_paths])\n        for file_path in pkl_file_paths:\n            self.data.extend(pickle.load(open(file_path, 'rb')))\n        self.transform = transform\n        self.use_attr = use_attr\n        self.no_img = no_img\n        self.uncertain_label = uncertain_label\n        self.image_dir = image_dir\n        self.n_class_attr = n_class_attr\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_data = self.data[idx]\n        img_path = img_data['img_path']\n        \n        # Trim unnecessary paths\n        try:\n            idx = img_path.split('/').index('CUB_200_2011')\n            if self.image_dir != 'images':\n                img_path = '/'.join([self.image_dir] + img_path.split('/')[idx+1:])\n                img_path = img_path.replace('images/', '')\n            else:\n                img_path = '/'.join(img_path.split('/')[idx:])\n            img = Image.open(img_path).convert('RGB')\n        except:\n            #img_path_split = img_path.split('/')\n            #split = 'train' if self.is_train else 'test'\n            #img_path = '/'.join(img_path_split[:2] + [split] + img_path_split[2:])\n            \n            img = Image.open(img_path).convert('RGB')\n\n        class_label = img_data['class_label']\n        if self.transform:\n            img = self.transform(img)\n\n        if self.use_attr:\n            if self.uncertain_label:\n                attr_label = img_data['uncertain_attribute_label']\n            else:\n                attr_label = img_data['attribute_label']\n            if self.no_img:\n                if self.n_class_attr == 3:\n                    one_hot_attr_label = np.zeros((N_ATTRIBUTES, self.n_class_attr))\n                    one_hot_attr_label[np.arange(N_ATTRIBUTES), attr_label] = 1\n                    return one_hot_attr_label, class_label\n                else:\n                    return attr_label, class_label\n            else:\n                return img, class_label, attr_label\n        else:\n            return img, class_label\n\n\nclass ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n    Arguments:\n        indices (list, optional): a list of indices\n        num_samples (int, optional): number of samples to draw\n    \"\"\"\n\n    def __init__(self, dataset, indices=None):\n        # if indices is not provided,\n        # all elements in the dataset will be considered\n        self.indices = list(range(len(dataset))) \\\n            if indices is None else indices\n\n        # if num_samples is not provided,\n        # draw `len(indices)` samples in each iteration\n        self.num_samples = len(self.indices)\n\n        # distribution of classes in the dataset\n        label_to_count = {}\n        for idx in self.indices:\n            label = self._get_label(dataset, idx)\n            if label in label_to_count:\n                label_to_count[label] += 1\n            else:\n                label_to_count[label] = 1\n\n        # weight for each sample\n        weights = [1.0 / label_to_count[self._get_label(dataset, idx)]\n                   for idx in self.indices]\n        self.weights = torch.DoubleTensor(weights)\n\n    def _get_label(self, dataset, idx):  # Note: for single attribute dataset\n        return dataset.data[idx]['attribute_label'][0]\n\n    def __iter__(self):\n        idx = (self.indices[i] for i in torch.multinomial(\n            self.weights, self.num_samples, replacement=True))\n        return idx\n\n    def __len__(self):\n        return self.num_samples\n\ndef load_data(pkl_paths, use_attr, no_img, batch_size, uncertain_label=False, n_class_attr=2, image_dir='images', resampling=False, resol=299):\n    \"\"\"\n    Note: Inception needs (299,299,3) images with inputs scaled between -1 and 1\n    Loads data with transformations applied, and upsample the minority class if there is class imbalance and weighted loss is not used\n    NOTE: resampling is customized for first attribute only, so change sampler.py if necessary\n    \"\"\"\n    resized_resol = int(resol * 256/224)\n    is_training = any(['train.pkl' in f for f in pkl_paths])\n    if is_training:\n        transform = transforms.Compose([\n            #transforms.Resize((resized_resol, resized_resol)),\n            #transforms.RandomSizedCrop(resol),\n            transforms.ColorJitter(brightness=32/255, saturation=(0.5, 1.5)),\n            transforms.RandomResizedCrop(resol),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(), #implicitly divides by 255\n            transforms.Normalize(mean = [0.5, 0.5, 0.5], std = [2, 2, 2])\n            #transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ], std = [ 0.229, 0.224, 0.225 ]),\n            ])\n    else:\n        transform = transforms.Compose([\n            #transforms.Resize((resized_resol, resized_resol)),\n            transforms.CenterCrop(resol),\n            transforms.ToTensor(), #implicitly divides by 255\n            transforms.Normalize(mean = [0.5, 0.5, 0.5], std = [2, 2, 2])\n            #transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ], std = [ 0.229, 0.224, 0.225 ]),\n            ])\n\n    dataset = CUBDataset(pkl_paths, use_attr, no_img, uncertain_label, image_dir, n_class_attr, transform)\n    if is_training:\n        drop_last = True\n        shuffle = True\n    else:\n        drop_last = False\n        shuffle = False\n    if resampling:\n        sampler = BatchSampler(ImbalancedDatasetSampler(dataset), batch_size=batch_size, drop_last=drop_last)\n        loader = DataLoader(dataset, batch_sampler=sampler)\n    else:\n        loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n    return loader\n\ndef find_class_imbalance(pkl_file, multiple_attr=False, attr_idx=-1):\n    \"\"\"\n    Calculate class imbalance ratio for binary attribute labels stored in pkl_file\n    If attr_idx >= 0, then only return ratio for the corresponding attribute id\n    If multiple_attr is True, then return imbalance ratio separately for each attribute. Else, calculate the overall imbalance across all attributes\n    \"\"\"\n    imbalance_ratio = []\n    data = pickle.load(open(os.path.join(BASE_DIR, pkl_file), 'rb'))\n    n = len(data)\n    n_attr = len(data[0]['attribute_label'])\n    if attr_idx >= 0:\n        n_attr = 1\n    if multiple_attr:\n        n_ones = [0] * n_attr\n        total = [n] * n_attr\n    else:\n        n_ones = [0]\n        total = [n * n_attr]\n    for d in data:\n        labels = d['attribute_label']\n        if multiple_attr:\n            for i in range(n_attr):\n                n_ones[i] += labels[i]\n        else:\n            if attr_idx >= 0:\n                n_ones[0] += labels[attr_idx]\n            else:\n                n_ones[0] += sum(labels)\n    for j in range(len(n_ones)):\n        imbalance_ratio.append(total[j]/n_ones[j] - 1)\n    if not multiple_attr: #e.g. [9.0] --> [9.0] * 312\n        imbalance_ratio *= n_attr\n    return imbalance_ratio","metadata":{"execution":{"iopub.status.busy":"2023-04-01T11:15:36.805699Z","iopub.execute_input":"2023-04-01T11:15:36.806074Z","iopub.status.idle":"2023-04-01T11:15:36.837483Z","shell.execute_reply.started":"2023-04-01T11:15:36.806043Z","shell.execute_reply":"2023-04-01T11:15:36.836235Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"!pip install gdown","metadata":{"execution":{"iopub.status.busy":"2023-04-01T10:59:23.698826Z","iopub.execute_input":"2023-04-01T10:59:23.699385Z","iopub.status.idle":"2023-04-01T10:59:34.590150Z","shell.execute_reply.started":"2023-04-01T10:59:23.699347Z","shell.execute_reply":"2023-04-01T10:59:34.588936Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting gdown\n  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from gdown) (4.64.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from gdown) (3.9.0)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from gdown) (4.11.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.7/site-packages (from gdown) (2.28.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gdown) (1.16.0)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.26.14)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (3.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2022.12.7)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.7.1)\nInstalling collected packages: gdown\nSuccessfully installed gdown-4.7.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!gdown 1DUkovCVCUqYScle624llHyeUuC7t0SaN\n!gdown 1oRBQ7WY_9-qfWkxNC7ZGBW27wecfEy6f\n!gdown 1EsYvhX6aRDELjoKpcRo3fuLo-aKF6DI9\n!gdown 1ir5HukW2XO25GWqx3jV-gKZWWi6Zg_rw","metadata":{"execution":{"iopub.status.busy":"2023-04-01T10:59:34.593120Z","iopub.execute_input":"2023-04-01T10:59:34.593495Z","iopub.status.idle":"2023-04-01T10:59:59.284179Z","shell.execute_reply.started":"2023-04-01T10:59:34.593451Z","shell.execute_reply":"2023-04-01T10:59:59.282944Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading...\nFrom (uriginal): https://drive.google.com/uc?id=1DUkovCVCUqYScle624llHyeUuC7t0SaN\nFrom (redirected): https://drive.google.com/uc?id=1DUkovCVCUqYScle624llHyeUuC7t0SaN&confirm=t&uuid=8d901508-fabc-48f9-8eb2-5ca2509ee65c\nTo: /kaggle/working/images.zip\n100%|██████████████████████████████████████| 1.13G/1.13G [00:15<00:00, 72.5MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1oRBQ7WY_9-qfWkxNC7ZGBW27wecfEy6f\nTo: /kaggle/working/kaggle_train.pkl\n100%|█████████████████████████████████████████| 497k/497k [00:00<00:00, 114MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1EsYvhX6aRDELjoKpcRo3fuLo-aKF6DI9\nTo: /kaggle/working/kaggle_test.pkl\n100%|████████████████████████████████████████| 601k/601k [00:00<00:00, 90.8MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1ir5HukW2XO25GWqx3jV-gKZWWi6Zg_rw\nTo: /kaggle/working/kaggle_val.pkl\n100%|████████████████████████████████████████| 124k/124k [00:00<00:00, 63.0MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import zipfile\n\nwith zipfile.ZipFile('./images.zip',\"r\") as z:\n    z.extractall()","metadata":{"execution":{"iopub.status.busy":"2023-04-01T10:59:59.287672Z","iopub.execute_input":"2023-04-01T10:59:59.288120Z","iopub.status.idle":"2023-04-01T11:00:05.729252Z","shell.execute_reply.started":"2023-04-01T10:59:59.288073Z","shell.execute_reply":"2023-04-01T11:00:05.727912Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Dataset loader from bottleneck concept script","metadata":{}},{"cell_type":"code","source":"\n\ntrain_data_path=\"/kaggle/working/kaggle_train.pkl\"\nval_data_path=\"/kaggle/working/kaggle_val.pkl\"\n\ntrain_loader = load_data([train_data_path], use_attr=False, no_img=False, batch_size=64, uncertain_label=False, image_dir=\"\", n_class_attr=0)\nval_loader = load_data([val_data_path], use_attr=False, no_img=False, batch_size=64, uncertain_label=False, image_dir=\"\", n_class_attr=0)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T11:15:46.388819Z","iopub.execute_input":"2023-04-01T11:15:46.389193Z","iopub.status.idle":"2023-04-01T11:15:46.402340Z","shell.execute_reply.started":"2023-04-01T11:15:46.389161Z","shell.execute_reply":"2023-04-01T11:15:46.401206Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\n\n# Load the pre-trained ResNet18 model\nmodel = models.resnet18(weights='DEFAULT')\n\n# Modify the last layer to match the number of classes in the CUB dataset\nnum_classes = 200\nmodel.fc = nn.Linear(model.fc.in_features, num_classes)\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score\n\n# Define the device to use for computation (GPU if available, else CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n\n\n# Define the number of epochs to train for\nnum_epochs = 50\n\n# Train the model\nfor epoch in range(num_epochs):\n    # Set the model to training mode\n    model.train()\n\n    # Train one epoch\n    train_loss = 0.0\n    train_acc = 0.0\n    train_size = 0\n    for images, labels in train_loader:\n        # Move the data to the device\n        model = model.to(device)\n        images, labels = images.to(device), labels.to(device)\n\n        # Zero the gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(images)\n        loss = loss_fn(outputs, labels)\n\n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n\n        # Update the training loss and accuracy\n        train_loss += loss.item() * images.size(0)\n        _, preds = torch.max(outputs, 1)\n        train_acc += accuracy_score(preds.cpu().numpy(), labels.cpu().numpy()) * images.size(0)\n        train_size += images.size(0)\n\n    train_loss /= train_size\n    train_acc /= train_size\n\n    # Set the model to evaluation mode\n    model.eval()\n    model = model.to(device)\n\n    # Evaluate one epoch\n    val_loss = 0.0\n    val_acc = 0.0\n    val_size = 0\n    with torch.no_grad():\n        for images, labels in val_loader:\n            # Move the data to the device\n            images, labels = images.to(device), labels.to(device)\n\n            # Forward pass\n            outputs = model(images)\n            loss = loss_fn(outputs, labels)\n\n            # Update the validation loss and accuracy\n            val_loss += loss.item() * images.size(0)\n            _, preds = torch.max(outputs, 1)\n            val_acc += accuracy_score(preds.cpu().numpy(), labels.cpu().numpy()) * images.size(0)\n            val_size += images.size(0)\n\n    val_loss /= val_size\n    val_acc /= val_size\n\n    # Print the results for this epoch\n    print(f\"Epoch {epoch + 1}/{num_epochs}: \"\n          f\"train_loss={train_loss:.4f} \"\n          f\"train_acc={train_acc:.4f} \"\n          f\"val_loss={val_loss:.4f} \"\n          f\"val_acc={val_acc:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-01T11:15:52.870014Z","iopub.execute_input":"2023-04-01T11:15:52.870380Z","iopub.status.idle":"2023-04-01T12:20:53.501231Z","shell.execute_reply.started":"2023-04-01T11:15:52.870348Z","shell.execute_reply":"2023-04-01T12:20:53.499948Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/44.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eac46bdf6c9a4dfa81cd9e1ad065e6f8"}},"metadata":{}},{"name":"stdout","text":"Epoch 1/50: train_loss=4.8614 train_acc=0.0490 val_loss=4.4651 val_acc=0.0576\nEpoch 2/50: train_loss=3.8086 train_acc=0.1347 val_loss=3.5806 val_acc=0.1628\nEpoch 3/50: train_loss=3.2003 train_acc=0.2329 val_loss=4.1831 val_acc=0.1469\nEpoch 4/50: train_loss=2.8089 train_acc=0.3095 val_loss=2.8962 val_acc=0.3005\nEpoch 5/50: train_loss=2.4553 train_acc=0.3872 val_loss=2.4875 val_acc=0.3623\nEpoch 6/50: train_loss=2.2043 train_acc=0.4316 val_loss=2.1558 val_acc=0.4232\nEpoch 7/50: train_loss=2.0520 train_acc=0.4797 val_loss=2.2456 val_acc=0.4407\nEpoch 8/50: train_loss=1.8653 train_acc=0.5154 val_loss=1.8994 val_acc=0.4858\nEpoch 9/50: train_loss=1.7281 train_acc=0.5515 val_loss=2.3273 val_acc=0.4207\nEpoch 10/50: train_loss=1.6099 train_acc=0.5834 val_loss=1.8611 val_acc=0.5159\nEpoch 11/50: train_loss=1.4773 train_acc=0.6128 val_loss=1.7721 val_acc=0.5551\nEpoch 12/50: train_loss=1.3985 train_acc=0.6313 val_loss=1.4822 val_acc=0.5860\nEpoch 13/50: train_loss=1.3269 train_acc=0.6569 val_loss=1.7152 val_acc=0.5684\nEpoch 14/50: train_loss=1.2642 train_acc=0.6776 val_loss=1.7849 val_acc=0.5417\nEpoch 15/50: train_loss=1.2428 train_acc=0.6772 val_loss=1.9150 val_acc=0.5317\nEpoch 16/50: train_loss=1.2069 train_acc=0.6860 val_loss=1.6560 val_acc=0.5659\nEpoch 17/50: train_loss=1.1135 train_acc=0.7050 val_loss=1.6836 val_acc=0.5801\nEpoch 18/50: train_loss=1.0200 train_acc=0.7329 val_loss=1.7121 val_acc=0.5710\nEpoch 19/50: train_loss=1.0545 train_acc=0.7291 val_loss=1.6505 val_acc=0.5952\nEpoch 20/50: train_loss=1.0393 train_acc=0.7304 val_loss=1.6437 val_acc=0.5968\nEpoch 21/50: train_loss=1.0147 train_acc=0.7384 val_loss=1.5788 val_acc=0.6110\nEpoch 22/50: train_loss=0.9392 train_acc=0.7555 val_loss=1.5079 val_acc=0.6210\nEpoch 23/50: train_loss=0.9658 train_acc=0.7618 val_loss=1.4436 val_acc=0.6219\nEpoch 24/50: train_loss=0.9009 train_acc=0.7701 val_loss=1.6392 val_acc=0.5960\nEpoch 25/50: train_loss=0.8616 train_acc=0.7832 val_loss=1.5687 val_acc=0.6227\nEpoch 26/50: train_loss=0.8289 train_acc=0.7855 val_loss=1.4219 val_acc=0.6461\nEpoch 27/50: train_loss=0.8243 train_acc=0.7895 val_loss=1.6165 val_acc=0.6344\nEpoch 28/50: train_loss=0.8350 train_acc=0.7842 val_loss=1.6862 val_acc=0.5876\nEpoch 29/50: train_loss=0.8101 train_acc=0.7965 val_loss=1.7471 val_acc=0.5985\nEpoch 30/50: train_loss=0.7734 train_acc=0.8013 val_loss=1.6299 val_acc=0.5968\nEpoch 31/50: train_loss=0.7461 train_acc=0.8070 val_loss=1.8235 val_acc=0.5885\nEpoch 32/50: train_loss=0.7792 train_acc=0.7969 val_loss=1.5782 val_acc=0.6411\nEpoch 33/50: train_loss=0.6802 train_acc=0.8290 val_loss=1.6490 val_acc=0.6394\nEpoch 34/50: train_loss=0.6875 train_acc=0.8275 val_loss=1.6234 val_acc=0.6311\nEpoch 35/50: train_loss=0.6798 train_acc=0.8294 val_loss=1.6110 val_acc=0.6377\nEpoch 36/50: train_loss=0.6929 train_acc=0.8281 val_loss=1.6756 val_acc=0.6477\nEpoch 37/50: train_loss=0.6699 train_acc=0.8283 val_loss=1.7039 val_acc=0.6160\nEpoch 38/50: train_loss=0.7037 train_acc=0.8188 val_loss=1.7120 val_acc=0.6144\nEpoch 39/50: train_loss=0.6499 train_acc=0.8376 val_loss=1.7663 val_acc=0.6219\nEpoch 40/50: train_loss=0.6388 train_acc=0.8389 val_loss=1.7421 val_acc=0.6135\nEpoch 41/50: train_loss=0.6297 train_acc=0.8416 val_loss=1.8921 val_acc=0.5885\nEpoch 42/50: train_loss=0.6380 train_acc=0.8330 val_loss=1.6461 val_acc=0.6302\nEpoch 43/50: train_loss=0.6166 train_acc=0.8410 val_loss=1.6223 val_acc=0.6244\nEpoch 44/50: train_loss=0.6408 train_acc=0.8397 val_loss=1.9977 val_acc=0.5810\nEpoch 45/50: train_loss=0.6069 train_acc=0.8450 val_loss=1.8499 val_acc=0.6327\nEpoch 46/50: train_loss=0.6179 train_acc=0.8387 val_loss=1.7706 val_acc=0.6319\nEpoch 47/50: train_loss=0.5740 train_acc=0.8501 val_loss=1.6200 val_acc=0.6444\nEpoch 48/50: train_loss=0.6015 train_acc=0.8530 val_loss=1.7777 val_acc=0.6152\nEpoch 49/50: train_loss=0.6254 train_acc=0.8402 val_loss=1.8991 val_acc=0.6177\nEpoch 50/50: train_loss=0.6398 train_acc=0.8334 val_loss=1.8171 val_acc=0.6119\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'resnet18_trained.pth')","metadata":{"execution":{"iopub.status.busy":"2023-04-01T12:20:53.503565Z","iopub.execute_input":"2023-04-01T12:20:53.504124Z","iopub.status.idle":"2023-04-01T12:20:53.580720Z","shell.execute_reply.started":"2023-04-01T12:20:53.504084Z","shell.execute_reply":"2023-04-01T12:20:53.579724Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Saliency maps with different data loader","metadata":{}}]}